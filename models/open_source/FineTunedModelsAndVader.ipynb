{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa668856",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fsspec==2024.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b24ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Bibliotecas padrão\n",
    "import collections\n",
    "import itertools\n",
    "import json\n",
    "import pickle as pkl\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Visualização de dados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Outras bibliotecas\n",
    "import unidecode\n",
    "\n",
    "# Processamento de linguagem natural\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from spacy.lang.pt import Portuguese\n",
    "import spacy\n",
    "\n",
    "# Importando bibliotecas necessárias\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31dfe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"CommentsReddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f72d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remocao de urls etc com regex\n",
    "\n",
    "df.Comentário = list(\n",
    "    map(\n",
    "        lambda x: re.sub(\n",
    "            r\"[(http(s)?):\\/\\/(www\\.)?a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\",\n",
    "            \"\",\n",
    "            x,\n",
    "        ),\n",
    "        df.Comentário.values,\n",
    "    )\n",
    ")\n",
    "\n",
    "df.Comentário = list(\n",
    "    map(\n",
    "        lambda x: re.sub(\n",
    "            r\"(\\n)|(\\t)|(\\s{2,})\",\n",
    "            \" \",\n",
    "            x,\n",
    "        ),\n",
    "        df.Comentário.values,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc2cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adicionando palavras indesejaveis nos topicos\n",
    "\n",
    "stop_list = ['kkkkk','kkkkkkk','kkkkkk','kkkk','kkkkkkkk','kkkkkkkkk','kkk','kkkkkkkkkk']\n",
    "stop_words=stopwords.words('portuguese')\n",
    "stop_words.extend(stop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42494f7a",
   "metadata": {},
   "source": [
    "# VADER/LeiA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba200086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from leia import SentimentIntensityAnalyzer\n",
    "s = SentimentIntensityAnalyzer()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ee6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_i = SentimentIntensityAnalyzer()\n",
    "def vadar_sentiment(text):\n",
    "    return sent_i.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorise_sentiment(sentiment, neg_threshold=-0.05, pos_threshold=0.05):\n",
    "    if sentiment < neg_threshold:\n",
    "        label = 'negative'\n",
    "    elif sentiment > pos_threshold:\n",
    "        label = 'positive'\n",
    "    else:\n",
    "        label = 'neutral'\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f6764",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['coef'] = df['Comentário'].apply(vadar_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f691415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentimentoLeia'] = df['coef'].apply(categorise_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b37677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o mapeamento\n",
    "label_mapping = {\"negative\": 0, \"positive\": 2, \"neutral\": 1}\n",
    "\n",
    "# Aplicar o mapeamento apenas às colunas especificadas\n",
    "columns_to_encode = ['sentimentoLeia']\n",
    "df = df.copy()\n",
    "df[columns_to_encode] = df[columns_to_encode].applymap(lambda x: label_mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4665ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator1_g1_labels =  df['class_result'].tolist()\n",
    "annotator2_g2_labels =  df['sentimentoLeia'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e485765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Calculate Cohen's Kappa between annotators 1 and 2\n",
    "kappa_1_2 = cohen_kappa_score(\n",
    "    annotator1_g1_labels,\n",
    "    annotator2_g2_labels\n",
    ")\n",
    "\n",
    "kappa_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c88247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(annotator1_g1_labels, annotator2_g2_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26e766",
   "metadata": {},
   "source": [
    "# Pysentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86999ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pysentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c0179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "stop_words = stopwords.words('portuguese')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, stop_words):\n",
    "    words = text.split()\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3352fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Comentário'] = df['Comentário'].apply(remove_stopwords, stop_words=stop_words)\n",
    "df['Comentário'] = df['Comentário'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35075ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentimiento import create_analyzer\n",
    "\n",
    "analyzer = create_analyzer(task=\"sentiment\", lang=\"pt\")\n",
    "\n",
    "def get_top_sentiment(comment):\n",
    "    analysis = analyzer.predict(comment)\n",
    "    return max(analysis.probas, key=analysis.probas.get)\n",
    "\n",
    "df['pysentimiento'] = df['Comentário'].apply(get_top_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ed2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pysentimiento'] = df['pysentimiento'].replace({'POS': 2, 'NEG': 0, 'NEU': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2daadaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator1_g1_labels =  df['class_result'].tolist()\n",
    "annotator2_g2_labels =  df['pysentimiento'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706d7c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_1_2 = cohen_kappa_score(\n",
    "    annotator1_g1_labels,\n",
    "    annotator2_g2_labels\n",
    ")\n",
    "\n",
    "kappa_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a71170",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(annotator1_g1_labels, annotator2_g2_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47065125",
   "metadata": {},
   "source": [
    "# XLM-RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39629dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel, AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from collections import defaultdict\n",
    "import urllib\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Carregue o modelo e o tokenizador\n",
    "roberta = \"cardiffnlp/xlm-roberta-base-tweet-sentiment-pt\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "model.config.max_position_embeddings\n",
    "\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "def get_sentiment_probabilities(tweet_text):\n",
    "    try:\n",
    "        encoded_tweet = tokenizer(tweet_text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_tweet)\n",
    "            predictions = outputs.logits\n",
    "            probabilities = softmax(predictions, axis=1).tolist()[0]\n",
    "\n",
    "        label_probabilities = {label: prob for label, prob in zip(labels, probabilities)}\n",
    "        return label_probabilities\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec059a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_probs'] = df['Comentário'].apply(get_sentiment_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7031e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_sentiment(sentiment_dict):\n",
    "    max_prob = max(sentiment_dict.values())\n",
    "    return [label for label, prob in sentiment_dict.items() if prob == max_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8862bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RoBERTa'] = df['sentiment_probs'].apply(get_max_sentiment)\n",
    "# Explodir a coluna 'sentimento' e contar os valores\n",
    "sentimento_counts = df.explode('RoBERTa')['RoBERTa'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82680bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('RoBERTa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d36e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o mapeamento\n",
    "label_mapping = {\"Negative\": 0, \"Positive\": 2, \"Neutral\": 1}\n",
    "\n",
    "# Aplicar o mapeamento apenas às colunas especificadas\n",
    "columns_to_encode = ['RoBERTa']\n",
    "df = df.copy()\n",
    "df[columns_to_encode] = df[columns_to_encode].applymap(lambda x: label_mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a541a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator1_g1_labels =  df['class_result'].tolist()\n",
    "annotator2_g2_labels =  df['RoBERTa'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6989cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_1_2 = cohen_kappa_score(\n",
    "    annotator1_g1_labels,\n",
    "    annotator2_g2_labels\n",
    ")\n",
    "\n",
    "kappa_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(annotator1_g1_labels, annotator2_g2_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
